{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "layout:     page\n",
    "title:      Homework 1, Question 8\n",
    "permalink:  /Z3o9P26CwTPZZMDXyWYDj3/hw1-q8/\n",
    "---\n",
    "\n",
    "# [CS 7643 Deep Learning - Homework 1][5]\n",
    "\n",
    "In this homework, we will learn how to implement backpropagation (or backprop) for \n",
    "“vanilla” neural networks (or Multi-Layer Perceptrons) and ConvNets.   \n",
    "You will begin by writing the forward and backward\n",
    "passes for different types of layers (including convolution and pooling),\n",
    "and then go on to train a shallow ConvNet on the CIFAR-10 dataset in Python.   \n",
    "Next you’ll learn to use [PyTorch][3], a popular open-source deep learning framework,\n",
    "and use it to replicate the experiments from before.\n",
    "\n",
    "This homework is divided into the following parts:\n",
    "\n",
    "- Implement a neural network and train a ConvNet on CIFAR-10 in Python.\n",
    "- Learn to use PyTorch and replicate previous experiments in PyTorch (2-layer NN, ConvNet on CIFAR-10).\n",
    "\n",
    "Download the starter code [here]({{site.baseurl}}/assets/f19cs7643_hw1_starter.zip).\n",
    "\n",
    "## Part 1\n",
    "\n",
    "Starter code for part 1 of the homework is available in the `1_cs231n` folder.\n",
    "\n",
    "### Setup\n",
    "\n",
    "Dependencies are listed in the `requirements.txt` file. If working with Anaconda, they should all be installed already.\n",
    "\n",
    "Download data.\n",
    "\n",
    "```bash\n",
    "cd 1_cs231n/cs231n/datasets\n",
    "./get_datasets.sh\n",
    "```\n",
    "\n",
    "Compile the Cython extension. From the `cs231n` directory, run the following.\n",
    "\n",
    "```bash\n",
    "python setup.py build_ext --inplace\n",
    "```\n",
    "\n",
    "### Q8.1: Softmax Regression (3 points)\n",
    "\n",
    "Work through `softmax.ipynb` and implement the Softmax classifier. Here is a brief summary and if you need a detailed tutorial to brush up your knowledge, [this](http://cs231n.github.io/linear-classify/) is a nice place.\n",
    "\n",
    "Before we go into the details of a classifier, let us assume that our training dataset consists of \\\\(N\\\\) instances \\\\(x\\_i \\in \\mathbb{R}^D \\\\) of dimensionality \\\\(D\\\\).\n",
    "Corresponding to each of the training instances,\n",
    "we have labels \\\\(y\\_i \\in \\{1,2,\\dotsc ,K \\}\\\\), where \\\\(K\\\\) is the number of classes.\n",
    "In this homework, we are using the CIFAR-10 database where \\\\(N=50,000\\\\), \\\\(K=10\\\\), \\\\(D= 32 \\times 32 \\times 3\\\\)\n",
    "(image of size  \\\\(32 \\times 32\\\\) with \\\\(3\\\\) channels - Red, Green, and Blue).\n",
    "\n",
    "Classification is the task of assigning a label to the input from a fixed set of categories or classes. A classifier consists of two important components:\n",
    "\n",
    "**Score function:** This maps every instance \\\\(x_i\\\\) to a vector \\\\(z\\_i\\\\) of dimensionality \\\\(K\\\\). Each of these entries represent the class scores for that image:\n",
    "\n",
    "\\\\[ z\\_i = Wx\\_i + b \\\\]\n",
    "\n",
    "Here, W is a matrix of weights of dimensionality \\\\(K \\times D\\\\) and b is a vector of bias terms of dimensionality \\\\(K \\times 1\\\\). The process of training is to find the appropriate values for W and b such that the score corresponding to the correct class is high. In order to do this, we need a function that evaluates the performance. Using this evaluation as feedback, the weights can be updated in the right 'direction' to improve the performance of the classifier.\n",
    "\n",
    "Before proceeding, we'll incorporate the bias term into \\\\(W\\\\), making it of dimensionality \\\\(K \\times (D+1)\\\\). Also let a superscript \\\\(j\\\\) denote the \\\\(j^{th}\\\\) element of \\\\(z\\_i\\\\) and \\\\(w\\_j\\\\) be the \\\\(j^{th}\\\\) row of W so that \\\\(z\\_i^j = w\\_j^Tx\\_i\\\\). Finally apply the softmax function to compute probabilities (for the \\\\(i\\\\)th example and \\\\(j\\\\)th class):\n",
    "\n",
    "\\\\[ p_i^j = \\frac{e^{z\\_i^{j}}}{\\sum\\_k e^{z^k\\_i}} \\\\]\n",
    "\n",
    "**Loss function:** This function quantifies the correspondence between the predicted scores and ground truth labels. Softmax regression uses the cross-entropy loss:\n",
    "\n",
    "\\\\[ L = - \\frac{1}{N}\\sum\\_{i=1}^{N}\\log \\left( p_i^{y_i} \\right) \\\\]\n",
    "\n",
    "If the weights are allowed to take values as high as possible, the model can overfit to the training data. To prevent this from happening a regularization term \\\\(R(W)\\\\) is added to the loss function. The regularization term is the squared some of the weight matrix \\\\(W\\\\). Mathematically,\n",
    "\n",
    "\\\\[ R(W) = \\sum\\_{k}\\sum\\_{l}W\\_{k,l}^2 \\\\]\n",
    "\n",
    "The final loss is\n",
    "\n",
    "\\\\[ \\mathcal{L}(W) = L(W) + R(W) \\\\]\n",
    "\n",
    "The regularization term \\\\(R(W)\\\\) is usually multiplied by the regularization strength \\\\(\\lambda\\\\) before adding it to the loss function. \\\\(\\lambda\\\\) is a hyper parameter which needs to be tuned so that the classifier generalizes well over the training set.\n",
    "\n",
    "The next step is to update the weight parts such that the loss is minimized. This is done by Stochastic Gradient Descent (SGD). The weight update is done as:\n",
    "\n",
    "\\\\[ W := W - \\eta \\nabla_W \\mathcal{L}(W) \\\\]\n",
    "\n",
    "Here, \\\\(\\nabla_W \\mathcal{L}\\\\) is the gradient of the loss function and the factor \\\\(\\eta\\\\) is the learning rate. SGD is usually performed by computing the gradient w.r.t. a randomly selected batch from the training set.\n",
    "This method is more efficient than computing the gradient w.r.t the whole training set before each update is performed.\n",
    "\n",
    "### Q8.2: Two-layer Neural Network (3 points)\n",
    "\n",
    "The IPython notebook `two_layer_net.ipynb` will walk you through implementing a\n",
    "two-layer neural network on CIFAR-10. You will write a hard-coded 2-layer\n",
    "neural network, implement its backward pass, and tune its hyperparameters.\n",
    "\n",
    "### Q8.3: Modular Neural Network (5 points)\n",
    "\n",
    "The IPython notebook `layers.ipynb` will walk you through a modular neural network implementation. You will implement the forward and backward passes of many\n",
    "different layer types, including convolution and pooling layers.\n",
    "\n",
    "### Q8.4: ConvNet on CIFAR-10 (2 points)\n",
    "\n",
    "The IPython notebook `convnet.ipynb` will walk you through the process of training\n",
    "a (shallow) convolutional neural network on CIFAR-10.\n",
    "\n",
    "**Deliverables**\n",
    "\n",
    "Zip the completed ipython notebooks and relevant files.\n",
    "\n",
    "PDF files created from each `.ipynb` using `nbconvert`.\n",
    "\n",
    "```bash\n",
    "cd 1_cs231n\n",
    "./collect_submission.sh\n",
    "```\n",
    "\n",
    "Submit the generated zip file `1_cs231n.zip` and PDF files as stated above.\n",
    "\n",
    "## Part 2\n",
    "\n",
    "This part is similar to the first part except that you will now be using [PyTorch][3] to \n",
    "implement the two-layer neural network and the convolutional neural network. In part 1\n",
    "you implemented core operations given significant scaffolding code. In part 2 these core\n",
    "operations are given by PyTorch and you simply need to figure out how to use them.\n",
    "\n",
    "If you haven't already, install PyTorch (__please use PyTorch vesion >=0.2__). This will probably be as simple as running the\n",
    "commands in the [Get Started][3] section of the PyTorch page, but if you run in to problems\n",
    "check out the [installation section][10] of the github README, search Google, or come to\n",
    "office hours. You may want to go through the [PyTorch Tutorial][12] before continuing.\n",
    "This homework is not meant to provide a complete overview of Deep Learning framework\n",
    "features or PyTorch features.\n",
    "\n",
    "You probably found that your layer implementations in Python were much slower than\n",
    "the optimized Cython version. Open-source frameworks are becoming more and more\n",
    "optimized and provide even faster implementations. Most of them take advantage of\n",
    "both GPUs, which can offer a significant speedup (e.g., 50x). A library of highly optimized Deep\n",
    "Learning operations from Nvidia called the [CUDA® Deep Neural Network library (cuDNN)][9]\n",
    "also helps.\n",
    "\n",
    "You will be using existing layers and hence, this part should be short and simple. To get\n",
    "started with PyTorch you could just jump in to the implementation below or read through\n",
    "some of the documentation below.\n",
    "\n",
    "- What is PyTorch and what distinguishes it from other DL libraries? (github [README][11])\n",
    "- PyTorch [Variables](http://pytorch.org/docs/master/autograd.html#variable) (needed for autodiff)\n",
    "- PyTorch [Modules](http://pytorch.org/docs/master/nn.html)\n",
    "- PyTorch [examples][8]\n",
    "\n",
    "The necessary files for this section are provided in the `2_pytorch` directory.\n",
    "You will only need to write code in `train.py` and in each file in the `models/` directory.\n",
    "\n",
    "### Q8.5: Softmax Classifier using PyTorch (2 points)\n",
    "\n",
    "The`softmax-classifier.ipynb` notebook will walk you through implementing a softmax\n",
    "classifier using PyTorch. Data loading and scaffolding for a train loop are provided.\n",
    "In `filter-viz.ipynb` you will load the trained model and extract its weight so they can be visualized.\n",
    "\n",
    "### Q8.6: Two-layer Neural Network using PyTorch (2 points)\n",
    "\n",
    "By now, you have an idea of working with PyTorch and may proceed to implementing a two-layer neural network. Go to \n",
    "`models/twolayernn.py` and complete the `TwoLayerNN` `Module`. Now train the neural network using\n",
    "\n",
    "```bash\n",
    "run_twolayernn.sh\n",
    "```\n",
    "    \n",
    "You will need to adjust hyperparameters in `run_twolayernn.sh` to achieve good performance.\n",
    "Use the code from `softmax-classifier.ipynb` to generate a __loss vs iterations__ plot for train\n",
    "and val and a __validation accuracy vs iterations__ plot. Save these plots as `twolayernn_lossvstrain.png` and `twolayernn_valaccuracy.png` respectively\n",
    "\n",
    "Make suitable modifications in `filter-viz.ipynb`\n",
    "and save visualizations of the weights of the first hidden layer called `twolayernn_gridfilt.png`.\n",
    "\n",
    "### Q8.7: ConvNet using PyTorch (2 points)\n",
    "\n",
    "Repeat the above steps for a convnet. Model code is in `models/convnet.py` and the script to train convnet is  in `run_convnet.sh`. Making suitable modifications to `softmax-classifier.ipynb` and `filter-viz.ipynb`, save the plots as `twolayernn_lossvstrain.png` and `twolayernn_valaccuracy.png` and the filters learned as `convnet_gridfilt.png`.\n",
    "\n",
    "## Deliverables\n",
    "\n",
    "Submit the results by uploading a zip file called `2_pytorch.zip`.\n",
    "\n",
    "PDF file called `2_pytorch.pdf` containing all saved `.png` files.\n",
    "\n",
    "```bash\n",
    "cd 2_pytorch/\n",
    "./collect_submission.sh\n",
    "```\n",
    "\n",
    "The following files should be included:\n",
    "\n",
    "1. Model implementations `models/*.py`\n",
    "2. Training code `train.py`\n",
    "3. All of the shell scripts used to train the 3 models (`run_softmax.sh`, `run_twolayernn.sh`, `run_convnet.sh`)\n",
    "3. Learning curves (loss) and validation accuracy plots from Q8.3 and Q8.4.\n",
    "4. The version of `filter-viz.ipynb` used to genepointsrate convnet filter visualizations\n",
    "5. PDF files created for each subquestion as per deliverables.\n",
    "6. Log files for each model with test accuracy reported at the bottom\n",
    "\n",
    "\n",
    "### Q8.8 Experiment (Extra credit for 4803, regular credits for 7643: 10 points)\n",
    "\n",
    "Experiment and try to get the best performance that you can on CIFAR-10 using a ConvNet.\n",
    "Submit your entry on a challenge hosted on [EvalAI](https://evalai.cloudcv.org/web/challenges/challenge-page/431/overview). The website will show a live leader board, so you can see how your implementation is doing compared to others. In order to prevent you from overfitting to the test data, the website limits the number of submissions to 3 per day, and only shows the leaderboard computed on 10% of the test data (so final standings may change). You will receive 5 pts regular credit for submitting something that beats chance, 5 points extra credit for beating the instructor/TA’s implementation.\n",
    "\n",
    "Evaluate your best model using `test.py` and upload the `predictions.csv` file on EvalAI. To participate, you will have to sign up on EvalAI using your gatech.edu email. Please tell us in your writeup `extra.md` what you tried. \n",
    "\n",
    "For getting better performance, some things you can try:  \n",
    "- Filter size: In part 1 we used 7x7; this makes pretty pictures but smaller filters may be more efficient\n",
    "- Number of filters: In part 1 we used 32 filters. Do more or fewer do better?\n",
    "- Network depth: Some good architectures to try include:\n",
    "    - [conv-relu-pool]xN - conv - relu - [affine]xM - [softmax or SVM]\n",
    "    - [conv-relu-pool]xN - [affine]xM - [softmax or SVM]\n",
    "    - [conv-relu-conv-relu-pool]xN - [affine]xM - [softmax or SVM]\n",
    "- Alternative update steps: AdaGrad, AdaDelta, Adam\n",
    "\n",
    "**Deliverables**\n",
    "\n",
    "Be sure to include the following in the `2_pytorch.zip` file\n",
    "\n",
    "- Model definition file `models/mymodel.py`\n",
    "- Training log, loss plot, and validation accuracy plot as above\n",
    "- List and describe all that you tried in a text file called `extra.md`\n",
    "- Name, Email ID, and Best Accuracy in `extra.md`\n",
    "\n",
    "References:\n",
    "\n",
    "1. [CS231n Convolutional Neural Networks for Visual Recognition][2]\n",
    "\n",
    "[2]: http://cs231n.stanford.edu/\n",
    "[3]: http://pytorch.org/\n",
    "[4]: http://bvlc.eecs.berkeley.edu/\n",
    "[5]: https://www.cc.gatech.edu/classes/AY2019/cs7643_fall/\n",
    "[8]: https://github.com/pytorch/examples\n",
    "[9]: https://developer.nvidia.com/cudnn\n",
    "[10]: https://github.com/pytorch/pytorch#installation\n",
    "[11]: https://github.com/pytorch/pytorch\n",
    "[12]: http://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "&#169; 2019 Georgia Tech\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "layout:     page\n",
    "title:      Homework 1, Question 8\n",
    "permalink:  /Z3o9P26CwTPZZMDXyWYDj3/hw1-q8-setup/\n",
    "---\n",
    "\n",
    "# Setup Instructions\n",
    "\n",
    "Follow the instructions before you start working on the homework. These steps will be useful for later homeworks as well.\n",
    "\n",
    "### Getting Started\n",
    "\n",
    "In this course, we will be using python often (most assignments will need a good amount of python). We have tested the code with Python 3.7.1 and recommend using a virtual environment with Python 3.7.1 for the assignments. \n",
    "\n",
    "#### Anaconda\n",
    "\n",
    "Although many distributions of python are available, we recommend that you use the [Anaconda Python](https://store.continuum.io/cshop/anaconda/). Here are the advantages of using Anaconda:\n",
    "\n",
    "- Easy seamless install of [python packages](http://docs.continuum.io/anaconda/pkg-docs) (most come standard)\n",
    "- It does not need root access to install new packages\n",
    "- Supported by Linux, OS X and Windows\n",
    "- Free!\n",
    "\n",
    "We suggest that you use either Linux (preferably Ubuntu) or OS X.\n",
    "Follow the instructions [here](http://docs.continuum.io/anaconda/install) to install Anaconda python.\n",
    "Remember to make Anaconda python the default python on your computer.\n",
    "Common issues are addressed here in the  [FAQ](http://docs.continuum.io/anaconda/faq).\n",
    "\n",
    "#### Python\n",
    "If you are comfortable with python, you can skip this section! \n",
    "\n",
    "If you are new to python and have sufficient programming experience in using languages like C/C++, MATLAB, etc., you should be able to grasp the basic workings of python necessary for this course easily.\n",
    "\n",
    "We will be using the [Numpy](http://www.numpy.org/) package extensively as it is the fundamental package for scientific computing providing support for array operations, linear algebra, etc. A good tutorial to get you started is [here](http://cs231n.github.io/python-numpy-tutorial/). For those comfortable with the operations of MATLAB, [this](https://docs.scipy.org/doc/numpy-dev/user/numpy-for-matlab-users.html) might prove useful.\n",
    "\n",
    "For some assignments, we will be using the [Jupyter Notebook](https://jupyter.org/).\n",
    "Jupyter is a web app for interactive computing developed originally developed as part of the [IPython](https://ipython.org/) interactive shell.\n",
    "The notebook is a useful environment where text can be embedded with code enabling us to set a flow while you do the assignments.\n",
    "\n",
    "We recommend creating a virtual environment through anaconda for the class with Python=3.6.1 and all the other dependencies installed by running:\n",
    "```sh\n",
    "conda create -n cs7643 python=3.6.1 anaconda\n",
    "conda activate cs7643\n",
    "```\n",
    "\n",
    "If you have setup your virtual environment correctly, you should be able to start the Jupyter notebook environment with:\n",
    "\n",
    "```sh\n",
    "jupyter notebook\n",
    "```\n",
    "\n",
    "Now you should be able to see the Jupyter home page when you navigate to `http://localhost:8888/` in your browser.\n",
    "It shows you a listing of files in the directory you ran the `jupyter notebook` command from and allows you to create new notebooks.\n",
    "Jupyter notebook files have `.ipynb` extensions.\n",
    "\n",
    "You will train the classifiers you create on images in the [CIFAR-10 dataset](http://www.cs.toronto.edu/~kriz/cifar.html).\n",
    "\n",
    "#### Downloading the starter code\n",
    "Download and unzip the [starter code](https://www.cc.gatech.edu/classes/AY2020/cs7643_fall/assets/hw1_q8_starter.zip) then start a Jupyter notebook in the resulting directory.\n",
    "CIFAR-10 is a toy dataset with 60000 images of size 32 X 32, belonging to 10 classes.\n",
    "You need to implement various classifiers as a part of this homework.\n",
    "\n",
    "This homework is based on [assignment 1](http://cs231n.github.io/assignments2017/assignment1/) of the CS231n course at Stanford.\n",
    "\n",
    "#### Getting the dataset\n",
    "\n",
    "Make sure you are connected to the internet. Navigate to the `cs231n/datasets` folder and run the following:\n",
    "\n",
    "```sh\n",
    "./get_datasets.sh\n",
    "```\n",
    "\n",
    "This script will download the python version of the database for you and put it in `cs231n/datasets/cifar-10-batches-py` folder.\n",
    "\n",
    "Now you can proceed to solve questions in HW1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
